HVGNet: Hybrid Vision-Graph Network for Hyperspectral Image Classification in Crop Disease DetectionThis repository contains the official implementation of the method proposed in the article:"A Novel Hybrid Vision-Graph Network (HVGNet) for Hyperspectral Image Classification in Crop Disease Detection"Jiangsheng Gui*, Mandol Md SharifSchool of Computer Science and Technology, Zhejiang Sci-Tech University, Hangzhou 310018, China*Correspondence: jsgui@zstu.edu.cn | Tel.: +86-0571-8684-3320📝 Note: The paper is currently under review. The DOI will be added here once the paper is published.🌱 Project OverviewHVGNet is a novel hybrid deep learning architecture that integrates Vision Transformers (ViT) for robust spectral-spatial feature extraction, Graph Neural Networks (GNN) for modeling non-Euclidean relationships between image regions, and K-Nearest Neighbors (KNN) for enhanced classification. This powerful combination aims to improve the accuracy and robustness of hyperspectral image classification, specifically for crop disease detection.This work contributes to the field of agricultural remote sensing and crop disease detection by improving classification precision in hyperspectral data analysis.🗃️ Repository Structure├── HVGNet.py         # Main Python file implementing the HVGNet architecture
├── HVGNet_demo.ipynb # Jupyter Notebook demo for model usage and testing
└── README.md         # Project documentation

🛠️ RequirementsPython 3.8-3.10TensorFlow or PyTorch (depending on implementation)NumPySciPyScikit-learnMatplotlibNetworkXOpenCVtqdmInstall dependencies with:# It's recommended to use 'uv pip' for faster and more reliable dependency management:
# pip install uv
uv pip install -r requirements.txt

📂 DatasetThis project was trained and tested using publicly available hyperspectral datasets.Details, links, and preprocessing steps will be added once the paper is published.🚀 UsageTo run the script:python HVGNet.py

To explore in a notebook:Open HVGNet_demo.ipynb in Jupyter or Google Colab for an interactive demo.📈 ResultsExperimental results and ablation studies show that HVGNet outperforms traditional CNN and GCN-based models in accuracy and robustness for hyperspectral image classification tasks. For full performance metrics, refer to the final published article.Below is a comparison of HVGNet's performance against other state-of-the-art models:MetricGNNPGNN-NetGATsEAGNNEfficientNetV3Swin TransformerViTHVGNetAccuracy0.6250.7850.8350.8850.9550.9780.97350.9821Precision0.63120.790.840.890.9560.97750.97410.9815Recall0.6250.7850.8350.8850.9550.9780.97350.9821F1 Score0.62280.7820.8320.8830.95450.97780.97340.9818MCC0.3850.650.750.80.940.9750.97080.9802Percentage Improvement of HVGNet over other models:Accuracy:HVGNet outperforms GNN by 57.14%HVGNet outperforms PGNN-Net by 25.11%HVGNet outperforms GATs by 17.62%HVGNet outperforms EAGNN by 10.97%HVGNet outperforms EfficientNetV3 by 2.84%HVGNet outperforms Swin Transformer by 0.42%HVGNet outperforms ViT by 0.88%Precision:HVGNet outperforms GNN by 55.50%HVGNet outperforms PGNN-Net by 24.24%HVGNet outperforms GATs by 16.85%HVGNet outperforms EAGNN by 10.28%HVGNet outperforms EfficientNetV3 by 2.67%HVGNet outperforms Swin Transformer by 0.41%HVGNet outperforms ViT by 0.75%Recall:HVGNet outperforms GNN by 57.14%HVGNet outperforms PGNN-Net by 25.11%HVGNet outperforms GATs by 17.62%HVGNet outperforms EAGNN by 10.97%HVGNet outperforms EfficientNetV3 by 2.84%HVGNet outperforms Swin Transformer by 0.42%HVGNet outperforms ViT by 0.88%F1 Score:HVGNet outperforms GNN by 57.66%HVGNet outperforms PGNN-Net by 25.55%HVGNet outperforms GATs by 18.01%HVGNet outperforms EAGNN by 11.19%HVGNet outperforms EfficientNetV3 by 2.86%HVGNet outperforms Swin Transformer by 0.41%HVGNet outperforms ViT by 0.86%MCC:HVGNet outperforms GNN by 154.60%HVGNet outperforms PGNN-Net by 50.80%HVGNet outperforms GATs by 30.69%HVGNet outperforms EAGNN by 22.53%HVGNet outperforms EfficientNetV3 by 4.28%HVGNet outperforms Swin Transformer by 0.53%HVGNet outperforms ViT by 0.97%👨‍💼 AcknowledgmentsI had the privilege to work as an assistant on this project under Mandol Md Sharif. It was a great experience contributing to smart agriculture and AI research.📌 CitationIf you use this work in your research, please cite:To be updated after paper publication.

📬 ContactJiangsheng Gui: jsgui@zstu.edu.cnMandol Md Sharif: (contact info optional)🔗 DOIDOI will be updated here after publication.
